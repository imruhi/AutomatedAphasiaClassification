{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-05T15:50:38.529205700Z",
     "start_time": "2024-11-05T15:50:38.497798100Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "import re\n",
    "from preprocess_data.preprocess import postprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28395e90730650f6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Section the preprocessed data into scenarios (merge sentences) and combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "aphasia_type = ['wernicke', 'conduction', 'anomic', 'not aphasic', 'control']\n",
    "fnames = ['../ab_data/processed_data/processeddata_'+x+'.csv' for x in aphasia_type]\n",
    "\n",
    "dfs = [pd.read_csv(x,encoding='utf8').dropna() for x in fnames]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T15:50:38.847530900Z",
     "start_time": "2024-11-05T15:50:38.534214200Z"
    }
   },
   "id": "31bc28430e02166f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wernicke: 63\n",
      "conduction: 150\n",
      "anomic: 277\n",
      "not aphasic: 89\n",
      "control: 346\n"
     ]
    }
   ],
   "source": [
    "# check if all files exist \n",
    "# wernicke=63, conduction=150, anomic=277, NA=89, control=346\n",
    "\n",
    "for type, df in zip(aphasia_type, dfs):\n",
    "    print(type + \": \" + str(len(df['source_file'].unique())))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T15:50:38.868433200Z",
     "start_time": "2024-11-05T15:50:38.854437100Z"
    }
   },
   "id": "e081946d161329b3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6173bf0706abc0ec",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T15:50:38.911564900Z",
     "start_time": "2024-11-05T15:50:38.868433200Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_scenario(data, label):\n",
    "    columns = ['source_file', 'scenario', 'preprocessed_text', 'label']\n",
    "    new_data = [] \n",
    "    text = \"\"\n",
    "    # n = -1\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        if index == 0:\n",
    "            prev_scenario = row[\"scenario\"] \n",
    "            prev_sourcefile = row[\"source_file\"]\n",
    "        current_scenario = row[\"scenario\"]\n",
    "        current_sourcefile = row[\"source_file\"]\n",
    "        \n",
    "        if (current_scenario == prev_scenario and \n",
    "                current_sourcefile == prev_sourcefile):\n",
    "            text += \" \" + row[\"preprocessed_text\"]\n",
    "            # n += 1\n",
    "            \n",
    "        else:\n",
    "            # n = 0\n",
    "            text = str(re.sub(' +', ' ', text)).lstrip().rstrip().replace('?','.').replace('!', '.')\n",
    "            # if len(re.findall('\\.',text)) == 3:\n",
    "            new_data.append([row[\"source_file\"], prev_scenario, text, label])\n",
    "            text = row[\"preprocessed_text\"]\n",
    "        \n",
    "        prev_scenario = current_scenario\n",
    "        prev_sourcefile = current_sourcefile\n",
    "            \n",
    "    return pd.DataFrame(new_data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "merge_dfs = []\n",
    "for type, df in zip(aphasia_type, dfs):\n",
    "    x = merge_scenario(df,type.upper())\n",
    "    merge_dfs.append(x)\n",
    "    # print(x)\n",
    "    x.to_csv('../ab_data/processed_data/processeddata_'+type+'_para.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T15:50:41.954686400Z",
     "start_time": "2024-11-05T15:50:38.884520900Z"
    }
   },
   "id": "8ed05836d3e38f34"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "for type in aphasia_type:\n",
    "    fp = '../ab_data/processed_data/processeddata_'+type+'_para.csv'\n",
    "    df = pd.read_csv(fp)\n",
    "    new_processed = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        strs = row['preprocessed_text'].split('.')\n",
    "        new_strs = ''\n",
    "        for x in strs:\n",
    "            # more than two consecutive fp/up\n",
    "            y = re.sub('(ufp[\\W\\s]+){3,}', 'UP3 ', x)\n",
    "            y = re.sub('(fp[\\W\\s]+){3,}', 'FP3 ', x)\n",
    "            # two consecutive fp/up\n",
    "            y = re.sub('(ufp[\\W\\s]+){2}', 'UP2 ', y)   \n",
    "            y = re.sub('(fp[\\W\\s]+){2}', 'FP2 ', y)\n",
    "            # one fp/up\n",
    "            y = re.sub('ufp', 'UP1 ', y)\n",
    "            y = re.sub('fp', 'FP1 ', y)\n",
    "            if y:\n",
    "                new_strs += y + '. '\n",
    "        new_strs = postprocess(new_strs)          \n",
    "        new_processed.append(str(new_strs))\n",
    "    \n",
    "    df['new_preprocessed_text'] = new_processed\n",
    "    df.to_csv(fp, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T15:50:44.324936400Z",
     "start_time": "2024-11-05T15:50:41.954686400Z"
    }
   },
   "id": "8db0cd79a354d7ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make datasets for classification/interpreting\n",
    "includes downsampling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c931c367faa2731"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936\n",
      "1848\n",
      "397\n",
      "723\n",
      "2167\n"
     ]
    }
   ],
   "source": [
    "conduction_df = pd.read_csv('../ab_data/processed_data/processeddata_conduction_para.csv')[['label', 'new_preprocessed_text', 'scenario']]\n",
    "anomic_df = pd.read_csv('../ab_data/processed_data/processeddata_anomic_para.csv')[['label', 'new_preprocessed_text', 'scenario']]\n",
    "wernicke_df = pd.read_csv('../ab_data/processed_data/processeddata_wernicke_para.csv')[['label', 'new_preprocessed_text', 'scenario']]\n",
    "not_aphasic_df = pd.read_csv('../ab_data/processed_data/processeddata_not aphasic_para.csv')[['label', 'new_preprocessed_text', 'scenario']]\n",
    "control_df = pd.read_csv('../ab_data/processed_data/processeddata_control_para.csv')[['label', 'new_preprocessed_text', 'scenario']]\n",
    "\n",
    "fp = \"../ab_data/experiment_data/\"\n",
    "print(len(conduction_df))\n",
    "print(len(anomic_df))\n",
    "print(len(wernicke_df))\n",
    "print(len(not_aphasic_df))\n",
    "print(len(control_df))\n",
    "\n",
    "# random scenarios are same across the dfs (random_state=72)\n",
    "\n",
    "# conduction vs anomic\n",
    "pd.concat([conduction_df, anomic_df]).sample(frac=1, random_state=42).to_csv(fp+\"conduction_anomic.csv\")\n",
    "\n",
    "# conduction vs anomic vs control\n",
    "pd.concat([conduction_df, anomic_df, control_df]).sample(frac=1, random_state=42).to_csv(fp+\"conduction_anomic_control.csv\")\n",
    "\n",
    "# control vs anomic\n",
    "pd.concat([control_df, anomic_df]).sample(frac=1, random_state=42).to_csv(fp+\"control_anomic.csv\")\n",
    "\n",
    "# wernicke vs anomic (downsample anomic to 522 random scenarios)\n",
    "pd.concat([wernicke_df, anomic_df.sample(n=522, random_state=72)]).sample(frac=1, random_state=42).to_csv(fp+\"wernicke_anomic.csv\")\n",
    "\n",
    "# wernicke vs anomic vs control (downsample control and anomic to 522 random scenarios)\n",
    "pd.concat([wernicke_df, anomic_df.sample(n=522, random_state=72), control_df.sample(n=522, random_state=72)]).sample(frac=1, random_state=42).to_csv(fp+\"wernicke_anomic_control.csv\")\n",
    "\n",
    "# control vs conduction \n",
    "pd.concat([control_df, conduction_df]).sample(frac=1, random_state=42).to_csv(fp+\"control_conduction.csv\")\n",
    "\n",
    "# control vs wernicke (downsample control to 522 random scenarios)\n",
    "pd.concat([control_df.sample(n=522, random_state=72), wernicke_df]).sample(frac=1, random_state=42).to_csv(fp+\"control_wernicke.csv\")\n",
    "\n",
    "# anomic vs conduction vs wernicke (downsample anomic and conduction to 522 random scenarios\n",
    "pd.concat([anomic_df.sample(n=522, random_state=72), conduction_df.sample(n=522, random_state=72), wernicke_df]).sample(frac=1, random_state=42).to_csv(fp+\"anomic_conduction_wernicke.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T15:50:44.809751800Z",
     "start_time": "2024-11-05T15:50:44.324936400Z"
    }
   },
   "id": "bbde6534942a8bd3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
