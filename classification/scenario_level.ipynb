{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9668d2b1341b259",
   "metadata": {
    "id": "e9668d2b1341b259",
    "ExecuteTime": {
     "end_time": "2024-11-05T16:16:47.137260300Z",
     "start_time": "2024-11-05T16:16:46.333767800Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "import sys\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from transformers import logging\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=UndefinedMetricWarning)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:16:47.153202900Z",
     "start_time": "2024-11-05T16:16:47.140203400Z"
    }
   },
   "id": "498275f61bb9c1cf"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f92633d613951ef",
   "metadata": {
    "id": "9f92633d613951ef",
    "outputId": "4e6e950b-2823-4a86-d814-9da9703d8c31",
    "ExecuteTime": {
     "end_time": "2024-11-05T16:16:47.196200900Z",
     "start_time": "2024-11-05T16:16:47.153202900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "test_env = False\n",
    "\n",
    "fp = \"../ab_data/experiment_data/\"\n",
    "model_fp = '../models/scenario_level/'\n",
    "\n",
    "dataset_fp = [fp+'conduction_anomic.csv', fp+'conduction_anomic_control.csv', \n",
    "              fp+'control_anomic.csv', fp+'wernicke_anomic.csv', \n",
    "              fp+'wernicke_anomic_control.csv', fp+'control_conduction.csv',\n",
    "              fp+'control_wernicke.csv', fp+'anomic_conduction_wernicke.csv']\n",
    "\n",
    "fnames = [model_fp+'conduction_anomic', model_fp+'conduction_anomic_control',\n",
    "          model_fp+'control_anomic', model_fp+'wernicke_anomic',\n",
    "          model_fp+'wernicke_anomic_control', model_fp+'control_conduction',\n",
    "          model_fp+'control_wernicke', model_fp+'anomic_conduction_wernicke']\n",
    "\n",
    "ids2labels = [{0: \"ANOMIC\", 1: \"CONDUCTION\"}, {0: \"CONTROL\", 1: \"ANOMIC\", 2: \"CONDUCTION\"},\n",
    "              {0: \"CONTROL\", 1: \"ANOMIC\"}, {0: \"ANOMIC\", 1: \"WERNICKE\"}, \n",
    "              {0: \"CONTROL\", 1: \"ANOMIC\", 2: \"WERNICKE\"}, {0: \"CONTROL\", 1: \"CONDUCTION\"},\n",
    "              {0: \"CONTROL\", 1:\"WERNICKE\"}, {0: \"CONDUCTION\", 1: \"ANOMIC\", 2: \"WERNICKE\"}]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:16:47.205537600Z",
     "start_time": "2024-11-05T16:16:47.171202200Z"
    }
   },
   "id": "5007f317ede01881"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dceb02e5146993dc",
   "metadata": {
    "id": "dceb02e5146993dc",
    "ExecuteTime": {
     "end_time": "2024-11-05T16:16:47.206809100Z",
     "start_time": "2024-11-05T16:16:47.188200400Z"
    }
   },
   "outputs": [],
   "source": [
    "# non-binary classification metrics\n",
    "def compute_metrics3(eval_pred):\n",
    "    metrics = [\"accuracy\", \"recall\", \"precision\", \"f1\"] #List of metrics to return\n",
    "    metric={}\n",
    "    for met in metrics:\n",
    "       metric[met] = evaluate.load(met)\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    metric_res={}\n",
    "\n",
    "    # micro, macro and weighted scores since it is a multi-class\n",
    "    # classification\n",
    "    for met in metrics:\n",
    "        if met != \"accuracy\":\n",
    "            metric_res[met+\"_micro\"] = round(metric[met].compute(predictions=predictions, references=labels, average=\"micro\")[met], 5)\n",
    "\n",
    "            metric_res[met+\"_macro\"] = round(metric[met].compute(predictions=predictions, references=labels, average=\"macro\")[met], 5)\n",
    "\n",
    "            metric_res[met+\"_weighted\"] = round(metric[met].compute(predictions=predictions, references=labels, average=\"weighted\")[met], 5)\n",
    "        else:\n",
    "            metric_res[met]=round(metric[met].compute(predictions=predictions, references=labels)[met], 5)\n",
    "    print(metric_res)\n",
    "    return metric_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# binary classification metrics\n",
    "def compute_metrics2(eval_pred):\n",
    "    metrics = [\"accuracy\", \"recall\", \"precision\", \"f1\"] #List of metrics to return\n",
    "    metric={}\n",
    "    for met in metrics:\n",
    "       metric[met] = evaluate.load(met)\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    metric_res={}\n",
    "    for met in metrics:\n",
    "       metric_res[met]=round(metric[met].compute(predictions=predictions,\n",
    "       references=labels)[met], 5)\n",
    "    print(metric_res)\n",
    "    return metric_res"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:16:47.214067300Z",
     "start_time": "2024-11-05T16:16:47.202659400Z"
    }
   },
   "id": "e413b40bb36e9aa1"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"new_preprocessed_text\"], padding=True, truncation=True, add_special_tokens=True, return_tensors=\"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:16:47.234066900Z",
     "start_time": "2024-11-05T16:16:47.215066900Z"
    }
   },
   "id": "da06235756874838"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def def_model(id2label_):\n",
    "    \"\"\"\n",
    "    Creates a distilbert uncased sequence classification model and tokenizer with new added tokens\n",
    "    :param id2label_: (dict) aphasic groups and their corresponding ids\n",
    "    :return: bert model and bert tokenizer\n",
    "    \"\"\"\n",
    "    label2id = {v: k for k, v in id2label_.items()}\n",
    "    tokenizer_ = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    if len(label2id) == 2:\n",
    "        model_ = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2, id2label=id2label_, label2id=label2id)\n",
    "    else:\n",
    "        model_ = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3, id2label=id2label_, label2id=label2id)\n",
    "        \n",
    "    added_tokens = tokenizer_.add_tokens([\"FP1\"], special_tokens=True)\n",
    "    added_tokens1 =  tokenizer_.add_tokens([\"UP1\"], special_tokens=True)\n",
    "    added_tokens2 = tokenizer_.add_tokens([\"FP2\"], special_tokens=True)\n",
    "    added_tokens3 =  tokenizer_.add_tokens([\"UP2\"], special_tokens=True)\n",
    "    added_tokens4 = tokenizer_.add_tokens([\"FP3\"], special_tokens=True)\n",
    "    added_tokens5 =  tokenizer_.add_tokens([\"UP3\"], special_tokens=True)\n",
    "    \n",
    "    print(\"We have added\", added_tokens + added_tokens1 + added_tokens2 + added_tokens3 + added_tokens4 +\n",
    "          added_tokens5, \"tokens\")\n",
    "    model_.resize_token_embeddings(len(tokenizer_))\n",
    "    \n",
    "    return model_, tokenizer_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:16:47.244066100Z",
     "start_time": "2024-11-05T16:16:47.233067900Z"
    }
   },
   "id": "ebc8a66ac6d2e218"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def def_dataset(dataset_filename_, id2label_, testing=False):\n",
    "    \"\"\"\n",
    "    Creates a 70 train/ 30 test dataset with stratification and with fixed randomization (seed=42) \n",
    "    :param dataset_filename_: (str) filepath of the datasest\n",
    "    :param id2label_: (dict) aphasic groups and their corresponding ids\n",
    "    :param testing: (bool) whether in testing or not\n",
    "    :return: tokenized and split dataset as a Dataset instance \n",
    "    \"\"\"\n",
    "    label2id = {v: k for k, v in id2label_.items()}\n",
    "    data = pd.read_csv(dataset_filename_).dropna()\n",
    "    data[\"label\"] = [label2id[x] for x in data[\"label\"]]\n",
    "    # for testing \n",
    "    if testing:\n",
    "        data = data[:100]\n",
    "    dataset = Dataset.from_pandas(data)\n",
    "    tokenized_data = dataset.map(preprocess_function, batched=True).with_format(\"torch\")\n",
    "    tokenized_data_split_ = tokenized_data.train_test_split(test_size=0.3, seed=42)\n",
    "    return tokenized_data_split_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:16:47.263640100Z",
     "start_time": "2024-11-05T16:16:47.248067Z"
    }
   },
   "id": "86e7afb8d77dc5d9"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def def_train_args(fname_):\n",
    "    \"\"\"\n",
    "    Creates the training args for the training procedure\n",
    "    :param fname_: (str) filepath to the model's save/log files\n",
    "    :return: the training arguments \n",
    "    \"\"\"\n",
    "    training_args_ = TrainingArguments(\n",
    "        output_dir=fname_+\"/train\",\n",
    "        logging_dir=fname+\"/logs\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3 if \"wernicke\" in fname_ else 5, # 3 for wernicke including, otherwise 5\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        use_cpu = False,\n",
    "        log_level = 'warning'\n",
    "    )\n",
    "    return training_args_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:16:47.275379300Z",
     "start_time": "2024-11-05T16:16:47.263640100Z"
    }
   },
   "id": "6514591df6e9724e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bdcaaf28048c7c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9452bbc8581b59",
   "metadata": {
    "id": "de9452bbc8581b59",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-11-05T16:16:47.280196700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: ['ANOMIC', 'CONDUCTION']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 6 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2780 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a95c829e3dee4ddf934425e54f1d0333"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/610 : < :, Epoch 0.01/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/53 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: ['CONTROL', 'ANOMIC', 'CONDUCTION']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 6 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "079142f72bbc4ff9be182a53221fbf83"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='1085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1085 : < :, Epoch 0.00/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/93 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: ['CONTROL', 'ANOMIC']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 6 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/4015 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0441a30f8e0545c78a88f0a6d044e7e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/880 : < :, Epoch 0.01/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train classifier\n",
    "\n",
    "for dataset_filename, fname, id2label in zip(dataset_fp, fnames, ids2labels):\n",
    "\n",
    "    print(f'Classifier: {list(id2label.values())}')\n",
    "    # define model, tokenizer, dataset and train args\n",
    "    model, tokenizer = def_model(id2label)\n",
    "    tokenized_data_split = def_dataset(dataset_filename, id2label, test_env)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    training_args = def_train_args(fname)\n",
    "\n",
    "    trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_data_split[\"train\"],\n",
    "            eval_dataset=tokenized_data_split[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics2 if len(id2label)==2 else compute_metrics3,\n",
    "\n",
    "        )\n",
    "\n",
    "    # saving train output to txtfile (overwrite existing one)\n",
    "    try:\n",
    "        os.remove(fname+\"/train_data.txt\")\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    orig_stdout = sys.stdout\n",
    "    sys.stdout = open(fname+\"/train_data.txt\",'wt')\n",
    "    print('Train results per epoch\\n')\n",
    "    # train model\n",
    "    trainer.train()\n",
    "    print('\\nTesting results\\n')\n",
    "    # evaluate model \n",
    "    trainer.evaluate()\n",
    "    sys.stdout.close()\n",
    "    sys.stdout=orig_stdout \n",
    "\n",
    "    # save trained model and tokenizer with new tokens \n",
    "    trainer.save_model(fname+\"/model\")\n",
    "    tokenizer.save_pretrained(fname+\"/tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfe6a614e8092a",
   "metadata": {
    "collapsed": false,
    "id": "1bfe6a614e8092a"
   },
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "941df8a481a4b300"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test the trained model on test set\n",
    "\n",
    "for dataset_filename, fname, id2label in zip(dataset_fp, fnames, ids2labels):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(fname+\"/tokenizer\", return_tensor=\"pt\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(fname+\"/model\")\n",
    "    model = model.to(device)\n",
    "    tokenized_data_split = def_dataset(dataset_filename, id2label, test_env)\n",
    "    sentences = tokenized_data_split[\"test\"][\"new_preprocessed_text\"]\n",
    "    true_labels = tokenized_data_split[\"test\"][\"label\"]\n",
    "    pred_labels = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\", add_special_tokens=True, max_length=512).input_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs).logits\n",
    "        predicted_class_id = logits.argmax().item()\n",
    "        pred_labels.append(predicted_class_id)\n",
    "    \n",
    "    true_labels = [id2label[x.to(torch.int64).item()] for x in true_labels]\n",
    "    pred_labels = [id2label[x] for x in pred_labels]\n",
    "    cm = confusion_matrix(true_labels, pred_labels, normalize='true')\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ConfusionMatrixDisplay.from_predictions(true_labels, pred_labels, normalize=\"true\", cmap=plt.cm.Blues, ax=ax)\n",
    "    plt.grid(False)\n",
    "    figname = '_'.join(list(id2label.values())).lower()\n",
    "    plt.savefig('scenario_level_output/'+figname+'.png')\n",
    "    \n",
    "    # saving test output to txtfile (overwrite existing one)\n",
    "    try:\n",
    "        os.remove('scenario_level_output/'+figname+'.txt')\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    orig_stdout = sys.stdout\n",
    "    sys.stdout = open('scenario_level_output/'+figname+'.txt','wt')\n",
    "    if len(id2label) == 2:\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = (2 * precision * recall)/ (precision+recall)\n",
    "        print(\"id2label=\" + str(id2label))\n",
    "        print(\"Accuracy: \" + str(round(accuracy_score(true_labels, pred_labels),5)))\n",
    "        print(\"Precision: \" + str(round(precision, 5)))\n",
    "        print(\"Recall: \" + str(round(recall,5)))\n",
    "        print(\"F1: \" + str(round(f1,5)))\n",
    "    else:\n",
    "        report = classification_report(true_labels, pred_labels, labels=np.unique(pred_labels), output_dict=True)\n",
    "        for k, v in report['macro avg'].items():\n",
    "            report['macro avg'][k] = round(v, 4)\n",
    "        for k, v in report['weighted avg'].items():\n",
    "            report['weighted avg'][k] = round(v, 4)\n",
    "\n",
    "        print(\"id2label=\" + str(id2label))\n",
    "        print(\"Accuracy: \" + str(round(accuracy_score(true_labels, pred_labels),4)))       \n",
    "        print(\"Macro avg: \" + str(report['macro avg']))\n",
    "        print(\"Weighted avg: \" + str(report['weighted avg']))\n",
    "    sys.stdout.close()\n",
    "    sys.stdout=orig_stdout \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "242952f825740145"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
